{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") \n",
    "\n",
    "from utils.paths import make_dir_line\n",
    "\n",
    "modality = 'u'\n",
    "project = 'nlp_course_w'\n",
    "data = make_dir_line(modality, project)\n",
    "\n",
    "raw = data('raw')\n",
    "processed = data('processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the csv into a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "news_articles_df = pd.read_csv(raw / 'articles1.csv')\n",
    "\n",
    "nRow, nCol = news_articles_df.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')\n",
    "\n",
    "news_articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the first unnames column (axis = 1 means it's column-wise operation. So Column 0 will be dropped)\n",
    "\n",
    "news_articles_df = news_articles_df.drop(news_articles_df.columns[0], axis = 1) \n",
    "\n",
    "nRow, nCol = news_articles_df.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')\n",
    "\n",
    "news_articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **EDA of the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of news publications names and article count in the dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color_list = list('rgbkymc')  #red, green, blue, black, etc.\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.xlabel(\"Publications\")\n",
    "plt.ylabel(\"Counts\")\n",
    "\n",
    "news_articles_df.publication.value_counts().plot(kind='bar', color=color_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of news article count per year\n",
    "\n",
    "news_articles_df['year'] = news_articles_df['year'].map(int).map(str)\n",
    "news_articles_df['year'] = pd.to_datetime(news_articles_df['year'], format='%Y').dt.strftime('%Y')\n",
    "news_articles_df = news_articles_df.sort_values(by='year')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Counts\")\n",
    "\n",
    "news_articles_df.year.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of news article count released monthly to observe if any pattern exists \n",
    "\n",
    "news_articles_df['month'] = news_articles_df['month'].map(int).map(str)\n",
    "news_articles_df['month'] = pd.to_datetime(news_articles_df['month'], format='%m').dt.strftime('%m')\n",
    "news_articles_df = news_articles_df.sort_values(by='month')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xlabel(\"month\")\n",
    "plt.ylabel(\"Counts\")\n",
    "\n",
    "news_articles_df.month.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of news article count per month-year\n",
    "\n",
    "news_articles_df['my'] = news_articles_df['month'].map(int).map(str) + '-' + news_articles_df['year'].map(int).map(str)\n",
    "news_articles_df['my'] = pd.to_datetime(news_articles_df['my'], format='%m-%Y').dt.strftime('%m-%Y')\n",
    "news_articles_df = news_articles_df.sort_values(by='my')\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.xlabel(\"month-year\")\n",
    "plt.ylabel(\"Counts\")\n",
    "\n",
    "news_articles_df.my.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a new dataframe with only the title column for faster operation \n",
    "\n",
    "title_df = news_articles_df['title']\n",
    "title_df = title_df.to_frame()\n",
    "\n",
    "title_df.sample(10, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing title length\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.xlabel(\"title length\")\n",
    "plt.ylabel(\"No. of articles\")\n",
    "\n",
    "title_df['title'].str.len().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part of Speech Tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df['title'] = title_df['title'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "title_df.sample(10, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining back the list of items into one string\n",
    "\n",
    "title_df['title'] = [' '.join(map(str, l)) for l in title_df['title']]\n",
    "title_df.sample(10, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df['title'] = title_df['title'].str.lower()\n",
    "title_df['title'] = title_df['title'].apply(lambda x: re.sub(r'[^\\w\\d\\s\\']+', '', x))\n",
    "\n",
    "title_df.sample(10, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk tokenization\n",
    "\n",
    "title_df['tokenized_title'] = title_df['title'].apply(word_tokenize)\n",
    "title_df.sample(10, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing Stop words libraries\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before removing checking occurances of stopwords in headline\n",
    "\n",
    "\n",
    "def plot_top_stopwords_barchart(text):\n",
    "    stop=set(stopwords.words('english'))\n",
    "    \n",
    "    new= text.str.split()\n",
    "    new=new.values.tolist()\n",
    "    corpus=[word for i in new for word in i]\n",
    "    from collections import defaultdict\n",
    "    dic=defaultdict(int)\n",
    "    for word in corpus:\n",
    "        if word in stop:\n",
    "            dic[word]+=1\n",
    "            \n",
    "    top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n",
    "    x,y=zip(*top)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.bar(x,y)\n",
    "\n",
    "plot_top_stopwords_barchart(title_df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from collections import  Counter\n",
    "\n",
    "def plot_top_non_stopwords_barchart(text):\n",
    "    stop=set(stopwords.words('english'))\n",
    "    \n",
    "    new= text.str.split()\n",
    "    new=new.values.tolist()\n",
    "    corpus=[word for i in new for word in i]\n",
    "\n",
    "    counter=Counter(corpus)\n",
    "    most=counter.most_common()\n",
    "    x, y=[], []\n",
    "    for word,count in most[:50]:\n",
    "        if (word not in stop):\n",
    "            x.append(word)\n",
    "            y.append(count)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.barplot(x=y,y=x)\n",
    "\n",
    "plot_top_non_stopwords_barchart(title_df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now removing stopwords \n",
    "\n",
    "title_df['tokenized_title'] = title_df['tokenized_title'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "title_df['tokenized_title_join'] = [' '.join(map(str, l)) for l in title_df['tokenized_title']]\n",
    "\n",
    "title_df.sample(10, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sake of simplicity, showing 10 titles based on the same seed value at 5\n",
    "tagged_stanzas = []\n",
    "tagged = []\n",
    "\n",
    "for word in title_df['tokenized_title'].sample(10, random_state = 5):\n",
    "  tagged_word = nltk.pos_tag(word)\n",
    "  print(tagged_word)\n",
    "\n",
    "  tagged_stanzas.append(tagged_word)\n",
    "\n",
    "# This format is needed for below visualizer as in takes only two values. If you skip this format, it might give you - \"too many values to unpack error\" \n",
    "tagged.append(tagged_stanzas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.text import PosTagVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "viz = PosTagVisualizer()\n",
    "viz.fit(tagged)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://miro.medium.com/max/700/1*bDMdq-EpaAr2fLTN84Ljgg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in tagged_stanzas:\n",
    "  print(sentence)\n",
    "  pattern = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "  NPChunker = nltk.RegexpParser(pattern) \n",
    "  result = NPChunker.parse(sentence)\n",
    "  result.pprint()\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "for sentence in title_df['title'].sample(5, random_state = 5):\n",
    "  sentence_doc = nlp(sentence)\n",
    "  \n",
    "  displacy.render(sentence_doc, style='dep', jupyter=True)\n",
    "  print(\"Sentence is: \", sentence_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ok_')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
